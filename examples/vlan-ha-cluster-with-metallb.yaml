---
# High Availability RKE2 Cluster with VLAN + MetalLB
#
# This example demonstrates:
# - 3-node HA control plane
# - VLAN networking for private node communication
# - MetalLB for LoadBalancer services (no CCM required)
# - Automatic providerID configuration
#
# Prerequisites:
# 1. Create SSH keys secret:
#    kubectl create secret generic latitude-ssh-keys --from-literal=authorized_keys="ssh-ed25519 ..."
# 2. Update: projectID, location, controlPlaneEndpoint, tlsSan, and credentials
#
# The controller automatically:
# - Creates/manages VLAN based on vlanConfig
# - Injects VLAN netplan configuration into servers
# - Assigns unique VLAN IPs to each server

apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: vlan-ha-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["10.244.0.0/16"]
    services:
      cidrBlocks: ["10.96.0.0/12"]
  controlPlaneEndpoint:
    host: "your-cluster-endpoint.example.com"  # Update with your DNS
    port: 6443
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: vlan-ha-cluster-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: LatitudeCluster
    name: vlan-ha-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: LatitudeCluster
metadata:
  name: vlan-ha-cluster
  namespace: default
spec:
  location: "SAO2"  # Update with your location
  projectRef:
    projectID: "proj_xxxxx"  # Update with your project ID
  vlanConfig:
    subnet: "10.10.0.0/24"  # VLAN subnet for private node communication
    # networkInterface: "eno2"  # Optional: defaults to eno2
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: LatitudeMachineTemplate
metadata:
  name: vlan-ha-cluster-control-plane
  namespace: default
spec:
  template:
    spec:
      plan: "c2-small-x86"
      operatingSystem: "ubuntu_24_04_x64_lts"
      sshKeySecretRef:
        name: latitude-ssh-keys
        namespace: default
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: vlan-ha-cluster-control-plane
  namespace: default
spec:
  replicas: 3
  version: v1.30.0+rke2r1
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: LatitudeMachineTemplate
    name: vlan-ha-cluster-control-plane
  agentConfig:
    nodeName: '{{ ds.meta_data.hostname }}'
  serverConfig:
    cni: calico
    cloudProviderName: external
    tlsSan:
      - your-cluster-endpoint.example.com  # Update to match controlPlaneEndpoint
      - 127.0.0.1
      - localhost
  registrationMethod: internal-first
  rolloutStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
  files:
    # Script to get server ID from Latitude API for providerID
    - path: /usr/local/bin/get-latitude-server-id.sh
      owner: root:root
      permissions: "0755"
      content: |-
        #!/bin/bash
        API_KEY="${LATITUDE_API_KEY:-}"
        PROJECT_ID="${LATITUDE_PROJECT_ID:-}"

        if [ -z "$API_KEY" ] || [ -z "$PROJECT_ID" ]; then
          echo "ERROR: LATITUDE_API_KEY and LATITUDE_PROJECT_ID must be set" >&2
          exit 1
        fi

        PUBLIC_IP=$(curl -s --connect-timeout 5 ifconfig.me || hostname -I | awk '{print $1}')
        RESPONSE=$(curl -s --connect-timeout 10 \
          -H "Authorization: Bearer $API_KEY" \
          "https://api.latitude.sh/servers?filter%5Bproject%5D=$PROJECT_ID")

        SERVER_ID=$(echo "$RESPONSE" | python3 -c "
        import sys, json
        data = json.load(sys.stdin)
        target_ip = '$PUBLIC_IP'
        for server in data.get('data', []):
            if server.get('attributes', {}).get('primary_ipv4') == target_ip:
                print(server['id'])
                sys.exit(0)
        sys.exit(1)
        " 2>/dev/null)

        if [ -n "$SERVER_ID" ]; then
          echo "$SERVER_ID"
        else
          exit 1
        fi

    # Latitude API credentials
    - path: /etc/latitude-credentials
      owner: root:root
      permissions: "0600"
      content: |-
        LATITUDE_API_KEY=your_api_key_here
        LATITUDE_PROJECT_ID=proj_xxxxx

  preRKE2Commands:
    # Firewall configuration
    - ufw --force enable
    - ufw default deny incoming
    - ufw default allow outgoing
    - ufw allow 22/tcp
    - ufw allow 6443/tcp
    - ufw allow 9345/tcp
    - ufw allow 2379:2380/tcp
    - ufw allow 10250/tcp
    - ufw allow 10259/tcp
    - ufw allow 10257/tcp
    - ufw allow 4789/udp
    - ufw allow 5473/tcp comment 'Calico Typha'

    # Kernel modules and networking
    - modprobe overlay
    - modprobe br_netfilter
    - sysctl -w net.bridge.bridge-nf-call-iptables=1
    - sysctl -w net.ipv4.ip_forward=1
    - swapoff -a

    # Configure providerID from Latitude API
    - |
      . /etc/latitude-credentials
      export LATITUDE_API_KEY LATITUDE_PROJECT_ID
      SERVER_ID=$(/usr/local/bin/get-latitude-server-id.sh 2>/dev/null || echo "")
      if [ -n "$SERVER_ID" ]; then
        mkdir -p /etc/rancher/rke2/config.yaml.d
        echo "kubelet-arg:" > /etc/rancher/rke2/config.yaml.d/99-provider-id.yaml
        echo "  - \"provider-id=latitude://$SERVER_ID\"" >> /etc/rancher/rke2/config.yaml.d/99-provider-id.yaml
        echo "Configured providerID: latitude://$SERVER_ID"
      fi

  postRKE2Commands:
    - export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
    - sleep 30
    # Remove cloud provider taint (required without CCM)
    - /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml taint nodes --all node.cloudprovider.kubernetes.io/uninitialized- || true
    # Install MetalLB
    - kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml
    - kubectl wait --namespace metallb-system --for=condition=ready pod --selector=app=metallb --timeout=120s || true
    - echo "Control plane ready - apply IPAddressPool and L2Advertisement to complete MetalLB setup"
